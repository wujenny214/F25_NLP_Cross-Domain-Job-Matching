{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "322cf491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>career_objective</th>\n",
       "      <th>skills</th>\n",
       "      <th>degree_names</th>\n",
       "      <th>major_field_of_studies</th>\n",
       "      <th>positions</th>\n",
       "      <th>responsibilities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>As a Data Analyst I always look into more inno...</td>\n",
       "      <td>['Machine Learning', 'Artificial Intelligence'...</td>\n",
       "      <td>['B.Tech', 'M.Tech']</td>\n",
       "      <td>[None, None]</td>\n",
       "      <td>['Data Analyst']</td>\n",
       "      <td>Mikrotik Router Configuration\\nOLT Device Setu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Financial and Accounting professional with exp...</td>\n",
       "      <td>['Power User of Microsoft Excel Epicor NetSuit...</td>\n",
       "      <td>['Bachelor of Business Administration']</td>\n",
       "      <td>['Accounting']</td>\n",
       "      <td>['Senior Accountant', 'Senior Accountant/Finan...</td>\n",
       "      <td>Design Creation\\nCAD Drawings\\nDesign Optimiza...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Fresher starting out with Business Analysis, a...</td>\n",
       "      <td>['Business Analyst', 'Data Analysis', 'Busines...</td>\n",
       "      <td>['BBA']</td>\n",
       "      <td>['N/A']</td>\n",
       "      <td>['Part-Time Analyst']</td>\n",
       "      <td>Full Stack Development\\nFront-end: ReactJS, Ne...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    career_objective  \\\n",
       "3  As a Data Analyst I always look into more inno...   \n",
       "4  Financial and Accounting professional with exp...   \n",
       "5  Fresher starting out with Business Analysis, a...   \n",
       "\n",
       "                                              skills  \\\n",
       "3  ['Machine Learning', 'Artificial Intelligence'...   \n",
       "4  ['Power User of Microsoft Excel Epicor NetSuit...   \n",
       "5  ['Business Analyst', 'Data Analysis', 'Busines...   \n",
       "\n",
       "                              degree_names major_field_of_studies  \\\n",
       "3                     ['B.Tech', 'M.Tech']           [None, None]   \n",
       "4  ['Bachelor of Business Administration']         ['Accounting']   \n",
       "5                                  ['BBA']                ['N/A']   \n",
       "\n",
       "                                           positions  \\\n",
       "3                                   ['Data Analyst']   \n",
       "4  ['Senior Accountant', 'Senior Accountant/Finan...   \n",
       "5                              ['Part-Time Analyst']   \n",
       "\n",
       "                                    responsibilities  \n",
       "3  Mikrotik Router Configuration\\nOLT Device Setu...  \n",
       "4  Design Creation\\nCAD Drawings\\nDesign Optimiza...  \n",
       "5  Full Stack Development\\nFront-end: ReactJS, Ne...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "resume_df = pd.read_csv(\"resume_cleaned_100.csv\")\n",
    "resume_df.iloc[3:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c68ccb5",
   "metadata": {},
   "source": [
    "# step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7d590259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building JD embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:05<00:00,  7.64it/s]\n",
      "Resumes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [08:21<00:00, 41.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " DONE! Saved to resume_job_top3.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "import ollama\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ===============================\n",
    "# Embedding model\n",
    "# ===============================\n",
    "embedder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# ===============================\n",
    "# Load data\n",
    "# ===============================\n",
    "jd_df = pd.read_excel(\"../1_data_cleaning/filtered_jd_sections2.xlsx\")\n",
    "jd_df[\"job_id\"] = jd_df.index.astype(int)\n",
    "VALID_JOB_IDS = set(jd_df[\"job_id\"].tolist())\n",
    "\n",
    "resume_df = pd.read_csv(\"../1_data_cleaning/resume_cleaned_100.csv\")\n",
    "resume_df = resume_df[\n",
    "    [\"career_objective\", \"skills\", \"degree_names\",\n",
    "     \"major_field_of_studies\", \"positions\", \"responsibilities\"]\n",
    "].iloc[8:20]\n",
    "resume_df[\"resume_id\"] = resume_df.index.astype(int)\n",
    "\n",
    "# ===============================\n",
    "# Utilities\n",
    "# ===============================\n",
    "def safe_json_loads(text):\n",
    "    try:\n",
    "        data = json.loads(text)\n",
    "        if isinstance(data, dict):\n",
    "            data = [data]\n",
    "        return data if isinstance(data, list) else []\n",
    "    except Exception:\n",
    "        try:\n",
    "            start = text.index(\"[\")\n",
    "            end = text.rindex(\"]\") + 1\n",
    "            return json.loads(text[start:end])\n",
    "        except Exception:\n",
    "            return []\n",
    "\n",
    "def summarize_text(text, max_length=900):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.strip()\n",
    "    return text[:max_length] if len(text) > max_length else text\n",
    "\n",
    "def build_resume_profile(row):\n",
    "    return f\"\"\"\n",
    "Career Objective:\n",
    "{summarize_text(row['career_objective'])}\n",
    "\n",
    "Skills:\n",
    "{summarize_text(row['skills'])}\n",
    "\n",
    "Degree Names:\n",
    "{summarize_text(row['degree_names'])}\n",
    "\n",
    "Major Field of Studies:\n",
    "{summarize_text(row['major_field_of_studies'])}\n",
    "\n",
    "Positions:\n",
    "{summarize_text(row['positions'])}\n",
    "\n",
    "Responsibilities:\n",
    "{summarize_text(row['responsibilities'])}\n",
    "\"\"\"\n",
    "\n",
    "# ===============================\n",
    "# Stage 1: SBERT coarse filter\n",
    "# ===============================\n",
    "def embed_text(text: str):\n",
    "    return embedder.encode(text if isinstance(text, str) else \"\")\n",
    "\n",
    "print(\"Building JD embeddings...\")\n",
    "jd_texts = jd_df[\"job_description\"].fillna(\"\").tolist()\n",
    "jd_embeddings = embedder.encode(jd_texts, show_progress_bar=True)\n",
    "\n",
    "def fast_filter_candidates(resume_text, top_k=20):\n",
    "    emb = embed_text(resume_text).reshape(1, -1)\n",
    "    sims = cosine_similarity(emb, jd_embeddings)[0]\n",
    "    top_ids = np.argsort(sims)[::-1][:top_k]\n",
    "    return jd_df.iloc[top_ids]\n",
    "\n",
    "# ===============================\n",
    "# Stage 2: LLM fine reasoning (Top-3)\n",
    "# ===============================\n",
    "LLM_MODEL = \"phi3:mini\"\n",
    "\n",
    "def build_prompt(resume_text, jd_batch):\n",
    "    jd_section = \"\"\n",
    "    for _, row in jd_batch.iterrows():\n",
    "        desc = summarize_text(row[\"job_description\"], max_length=900)\n",
    "        jd_section += f\"\"\"\n",
    "---\n",
    "Job ID: {row['job_id']}\n",
    "Job Title: {row['job_title']}\n",
    "Location: {row['location_cleaned']}\n",
    "Description:\n",
    "{desc}\n",
    "\"\"\"\n",
    "    return f\"\"\"\n",
    "You are a senior hiring expert.\n",
    "\n",
    "Given the following resume and job descriptions, identify the three best matching jobs overall.\n",
    "\n",
    "Base your judgment primarily on:\n",
    "- Skills and responsibilities alignment\n",
    "- Relevant experience\n",
    "- Educational background\n",
    "- Location fit (lowest weight)\n",
    "\n",
    "Return STRICT JSON ONLY in this format:\n",
    "[\n",
    "  {{\"job_id\": <best_job_id_1>, \"reason\": \"<brief explanation>\"}},\n",
    "  {{\"job_id\": <best_job_id_2>, \"reason\": \"<brief explanation>\"}},\n",
    "  {{\"job_id\": <best_job_id_3>, \"reason\": \"<brief explanation>\"}}\n",
    "]\n",
    "\n",
    "Resume:\n",
    "{resume_text}\n",
    "\n",
    "Job Descriptions:\n",
    "{jd_section}\n",
    "\"\"\"\n",
    "\n",
    "def score_batch_with_llm(resume_text, jd_batch, retry=3):\n",
    "    prompt = build_prompt(resume_text, jd_batch)\n",
    "    for _ in range(retry):\n",
    "        try:\n",
    "            resp = ollama.chat(\n",
    "                model=LLM_MODEL,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                options={\"num_ctx\": 4096}\n",
    "            )\n",
    "            txt = resp[\"message\"][\"content\"]\n",
    "            data = safe_json_loads(txt)\n",
    "            if data:\n",
    "                return data\n",
    "        except Exception as e:\n",
    "            print(\"‚ö†Ô∏è Error:\", e)\n",
    "            time.sleep(1)\n",
    "    return []\n",
    "\n",
    "# ===============================\n",
    "# Matching logic (Top-3)\n",
    "# ===============================\n",
    "def chunk_df(df, size=10):\n",
    "    for i in range(0, len(df), size):\n",
    "        yield df.iloc[i:i+size]\n",
    "\n",
    "def match_resume_to_jobs(resume_text):\n",
    "    filtered = fast_filter_candidates(resume_text, top_k=20)\n",
    "    best_jobs = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        futures = [\n",
    "            executor.submit(score_batch_with_llm, resume_text, batch)\n",
    "            for batch in chunk_df(filtered, size=10)\n",
    "        ]\n",
    "        for fut in as_completed(futures):\n",
    "            results = fut.result()\n",
    "            if not results:\n",
    "                continue\n",
    "            for r in results:\n",
    "                try:\n",
    "                    jid = int(r.get(\"job_id\", 0))\n",
    "                    if jid in VALID_JOB_IDS:\n",
    "                        best_jobs.append(jid)\n",
    "                except Exception:\n",
    "                    continue\n",
    "            if len(best_jobs) >= 3:\n",
    "                break\n",
    "\n",
    "    best_jobs = best_jobs[:3]\n",
    "    while len(best_jobs) < 3:\n",
    "        best_jobs.append(0)\n",
    "    return best_jobs\n",
    "\n",
    "# ===============================\n",
    "# Run all resumes\n",
    "# ===============================\n",
    "output = []\n",
    "\n",
    "for _, row in tqdm(resume_df.iterrows(), total=len(resume_df), desc=\"Resumes\"):\n",
    "    rid = row[\"resume_id\"]\n",
    "    resume_text = build_resume_profile(row)\n",
    "    top3 = match_resume_to_jobs(resume_text)\n",
    "\n",
    "    job_rows = [jd_df[jd_df[\"job_id\"] == jid].iloc[0] if jid in VALID_JOB_IDS else jd_df.iloc[0] for jid in top3]\n",
    "\n",
    "    output.append({\n",
    "        \"resume_id\": rid,\n",
    "        \"top1_match_job_id\": top3[0],\n",
    "        \"top1_match_job_title\": job_rows[0][\"job_title\"],\n",
    "        \"top1_match_job_location\": job_rows[0][\"location_cleaned\"],\n",
    "        \"top2_match_job_id\": top3[1],\n",
    "        \"top2_match_job_title\": job_rows[1][\"job_title\"],\n",
    "        \"top2_match_job_location\": job_rows[1][\"location_cleaned\"],\n",
    "        \"top3_match_job_id\": top3[2],\n",
    "        \"top3_match_job_title\": job_rows[2][\"job_title\"],\n",
    "        \"top3_match_job_location\": job_rows[2][\"location_cleaned\"]\n",
    "    })\n",
    "\n",
    "pd.DataFrame(output).to_excel(\"resume_job_top3.xlsx\", index=False)\n",
    "print(\"\\n DONE! Saved to resume_job_top3.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20746f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building JD embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:05<00:00,  7.15it/s]\n",
      "Resumes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:56<00:00, 28.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " DONE! Saved to resume_job_groundtrue.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import json\n",
    "# import numpy as np\n",
    "# from tqdm import tqdm\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "# import time\n",
    "# import ollama\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# # ===============================\n",
    "# # Embedding model\n",
    "# # ===============================\n",
    "# embedder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# # ===============================\n",
    "# # Load data\n",
    "# # ===============================\n",
    "# jd_df = pd.read_excel(\"../1_data_cleaning/filtered_jd_sections2.xlsx\")\n",
    "# jd_df[\"job_id\"] = jd_df.index.astype(int)\n",
    "# VALID_JOB_IDS = set(jd_df[\"job_id\"].tolist())\n",
    "\n",
    "# resume_df = pd.read_csv(\"../1_data_cleaning/resume_cleaned_100.csv\")\n",
    "# resume_df = resume_df[\n",
    "#     [\"career_objective\", \"skills\", \"degree_names\",\n",
    "#      \"major_field_of_studies\", \"positions\", \"responsibilities\"]\n",
    "# ].iloc[0:2]\n",
    "# resume_df[\"resume_id\"] = resume_df.index.astype(int)\n",
    "\n",
    "# # ===============================\n",
    "# # Utilities\n",
    "# # ===============================\n",
    "# def safe_json_loads(text):\n",
    "#     try:\n",
    "#         data = json.loads(text)\n",
    "#         if isinstance(data, dict):\n",
    "#             data = [data]\n",
    "#         return data if isinstance(data, list) else []\n",
    "#     except Exception:\n",
    "#         try:\n",
    "#             start = text.index(\"[\")\n",
    "#             end = text.rindex(\"]\") + 1\n",
    "#             return json.loads(text[start:end])\n",
    "#         except Exception:\n",
    "#             return []\n",
    "\n",
    "# def summarize_text(text, max_length=900):\n",
    "#     if not isinstance(text, str):\n",
    "#         return \"\"\n",
    "#     text = text.strip()\n",
    "#     return text[:max_length] if len(text) > max_length else text\n",
    "\n",
    "# def build_resume_profile(row):\n",
    "#     return f\"\"\"\n",
    "# Career Objective:\n",
    "# {summarize_text(row['career_objective'])}\n",
    "\n",
    "# Skills:\n",
    "# {summarize_text(row['skills'])}\n",
    "\n",
    "# Degree Names:\n",
    "# {summarize_text(row['degree_names'])}\n",
    "\n",
    "# Major Field of Studies:\n",
    "# {summarize_text(row['major_field_of_studies'])}\n",
    "\n",
    "# Positions:\n",
    "# {summarize_text(row['positions'])}\n",
    "\n",
    "# Responsibilities:\n",
    "# {summarize_text(row['responsibilities'])}\n",
    "# \"\"\"\n",
    "\n",
    "# # ===============================\n",
    "# # Stage 1: SBERT coarse filter\n",
    "# # ===============================\n",
    "# def embed_text(text: str):\n",
    "#     return embedder.encode(text if isinstance(text, str) else \"\")\n",
    "\n",
    "# print(\"Building JD embeddings...\")\n",
    "# jd_texts = jd_df[\"job_description\"].fillna(\"\").tolist()\n",
    "# jd_embeddings = embedder.encode(jd_texts, show_progress_bar=True)\n",
    "\n",
    "# def fast_filter_candidates(resume_text, top_k=20):\n",
    "#     emb = embed_text(resume_text).reshape(1, -1)\n",
    "#     sims = cosine_similarity(emb, jd_embeddings)[0]\n",
    "#     top_ids = np.argsort(sims)[::-1][:top_k]\n",
    "#     return jd_df.iloc[top_ids]\n",
    "\n",
    "# # ===============================\n",
    "# # Stage 2: LLM fine reasoning (Top-1 only)\n",
    "# # ===============================\n",
    "# LLM_MODEL = \"phi3:mini\"\n",
    "\n",
    "# def build_prompt(resume_text, jd_batch):\n",
    "#     jd_section = \"\"\n",
    "#     for _, row in jd_batch.iterrows():\n",
    "#         desc = summarize_text(row[\"job_description\"], max_length=900)\n",
    "#         jd_section += f\"\"\"\n",
    "# ---\n",
    "# Job ID: {row['job_id']}\n",
    "# Job Title: {row['job_title']}\n",
    "# Location: {row['location_cleaned']}\n",
    "# Description:\n",
    "# {desc}\n",
    "# \"\"\"\n",
    "#     return f\"\"\"\n",
    "# You are a senior hiring expert.\n",
    "\n",
    "# Given the following resume and job descriptions, identify the single best matching job overall.\n",
    "\n",
    "# Base your judgment primarily on:\n",
    "# - Skills and responsibilities alignment\n",
    "# - Relevant experience\n",
    "# - Educational background\n",
    "# - Location fit (lowest weight)\n",
    "\n",
    "# Return STRICT JSON ONLY in this format:\n",
    "# {{\n",
    "#   \"job_id\": <best_job_id>,\n",
    "#   \"reason\": \"<brief explanation>\"\n",
    "# }}\n",
    "\n",
    "# Resume:\n",
    "# {resume_text}\n",
    "\n",
    "# Job Descriptions:\n",
    "# {jd_section}\n",
    "# \"\"\"\n",
    "\n",
    "# def score_batch_with_llm(resume_text, jd_batch, retry=3):\n",
    "#     prompt = build_prompt(resume_text, jd_batch)\n",
    "#     for _ in range(retry):\n",
    "#         try:\n",
    "#             resp = ollama.chat(\n",
    "#                 model=LLM_MODEL,\n",
    "#                 messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "#                 options={\"num_ctx\": 4096}\n",
    "#             )\n",
    "#             txt = resp[\"message\"][\"content\"]\n",
    "#             data = safe_json_loads(txt)\n",
    "#             if data:\n",
    "#                 return data\n",
    "#         except Exception as e:\n",
    "#             print(\"‚ö†Ô∏è Error:\", e)\n",
    "#             time.sleep(1)\n",
    "#     return []\n",
    "\n",
    "# # ===============================\n",
    "# # Matching logic (Top-1 only)\n",
    "# # ===============================\n",
    "# def chunk_df(df, size=10):\n",
    "#     for i in range(0, len(df), size):\n",
    "#         yield df.iloc[i:i+size]\n",
    "\n",
    "# def match_resume_to_jobs(resume_text):\n",
    "#     filtered = fast_filter_candidates(resume_text, top_k=20)\n",
    "#     best_id = None\n",
    "\n",
    "#     with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "#         futures = [\n",
    "#             executor.submit(score_batch_with_llm, resume_text, batch)\n",
    "#             for batch in chunk_df(filtered, size=10)\n",
    "#         ]\n",
    "#         for fut in as_completed(futures):\n",
    "#             results = fut.result()\n",
    "#             if not results:\n",
    "#                 continue\n",
    "\n",
    "#             if isinstance(results, dict) or isinstance(results, (int, str)):\n",
    "#                 results = [results]\n",
    "\n",
    "#             for r in results:\n",
    "#                 jid = None\n",
    "#                 if isinstance(r, dict):\n",
    "#                     if \"job_id\" in r:\n",
    "#                         jid = r[\"job_id\"]\n",
    "#                     elif \"best_job_id\" in r:\n",
    "#                         jid = r[\"best_job_id\"]\n",
    "#                 elif isinstance(r, (int, str)):\n",
    "#                     jid = r\n",
    "\n",
    "#                 if jid is None:\n",
    "#                     continue\n",
    "\n",
    "#                 try:\n",
    "#                     jid = int(jid)\n",
    "#                 except ValueError:\n",
    "#                     continue\n",
    "\n",
    "#                 if jid in VALID_JOB_IDS:\n",
    "#                     best_id = jid\n",
    "#                     break\n",
    "\n",
    "#             if best_id is not None:\n",
    "#                 break\n",
    "\n",
    "#     return best_id if best_id is not None else 0\n",
    "\n",
    "# # ===============================\n",
    "# # Run all resumes\n",
    "# # ===============================\n",
    "# output = []\n",
    "\n",
    "# for _, row in tqdm(resume_df.iterrows(), total=len(resume_df), desc=\"Resumes\"):\n",
    "#     rid = row[\"resume_id\"]\n",
    "#     resume_text = build_resume_profile(row)\n",
    "#     best_jid = match_resume_to_jobs(resume_text)\n",
    "\n",
    "#     job_row = jd_df[jd_df[\"job_id\"] == best_jid].iloc[0] if best_jid in VALID_JOB_IDS else jd_df.iloc[0]\n",
    "\n",
    "#     output.append({\n",
    "#         \"resume_id\": rid,\n",
    "#         \"match_job_id\": best_jid,\n",
    "#         \"match_job_title\": job_row[\"job_title\"],\n",
    "#         \"match_job_location\": job_row[\"location_cleaned\"]\n",
    "#     })\n",
    "\n",
    "# pd.DataFrame(output).to_excel(\"resume_job_groundtrue.xlsx\", index=False)\n",
    "# print(\"\\n DONE! Saved to resume_job_groundtrue.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09a6113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building JD embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:06<00:00,  5.84it/s]\n",
      "Resumes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [01:25<00:00, 85.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DONE! Saved to resume_job_groundtrue.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import json\n",
    "# import numpy as np\n",
    "# from tqdm import tqdm\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "# import time\n",
    "# import ollama\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# # ========== Embedding model ==========\n",
    "# embedder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# # ========== Load JD data ==========\n",
    "# jd_df = pd.read_excel(\"jobdescription.xlsx\")\n",
    "# jd_df[\"job_id\"] = jd_df.index.astype(int)\n",
    "# VALID_JOB_IDS = set(jd_df[\"job_id\"].tolist())\n",
    "\n",
    "# # ========== Load Resume data ==========\n",
    "# resume_df = pd.read_csv(\"resume_cleaned_100.csv\")\n",
    "# resume_df = resume_df[\n",
    "#     ['career_objective', 'skills', 'degree_names',\n",
    "#      'major_field_of_studies', 'positions', 'responsibilities']\n",
    "# ].iloc[10:11]\n",
    "\n",
    "# #  resume_id\n",
    "# resume_df[\"resume_id\"] = resume_df.index.astype(int)\n",
    "\n",
    "# # =====================================================================\n",
    "# # JSON loader\n",
    "# # =====================================================================\n",
    "# def safe_json_loads(text):\n",
    "#     try:\n",
    "#         data = json.loads(text)\n",
    "#         if isinstance(data, dict):\n",
    "#             data = [data]\n",
    "#         return data if isinstance(data, list) else []\n",
    "#     except:\n",
    "#         try:\n",
    "#             start = text.index(\"[\")\n",
    "#             end = text.rindex(\"]\") + 1\n",
    "#             return json.loads(text[start:end])\n",
    "#         except:\n",
    "#             return []\n",
    "\n",
    "# # =====================================================================\n",
    "# # Text summarizer\n",
    "# # =====================================================================\n",
    "# def summarize_text(text, max_length=900):\n",
    "#     if not isinstance(text, str):\n",
    "#         return \"\"\n",
    "#     text = text.strip()\n",
    "#     return text[:max_length] if len(text) > max_length else text\n",
    "\n",
    "# # =====================================================================\n",
    "# # Build Resume text (UPDATED for your dataset)\n",
    "# # =====================================================================\n",
    "# def build_resume_profile(row):\n",
    "#     return f\"\"\"\n",
    "# Career Objective:\n",
    "# {summarize_text(row['career_objective'])}\n",
    "\n",
    "# Skills:\n",
    "# {summarize_text(row['skills'])}\n",
    "\n",
    "# Degree Names:\n",
    "# {summarize_text(row['degree_names'])}\n",
    "\n",
    "# Major Field of Studies:\n",
    "# {summarize_text(row['major_field_of_studies'])}\n",
    "\n",
    "# Positions:\n",
    "# {summarize_text(row['positions'])}\n",
    "\n",
    "# Responsibilities:\n",
    "# {summarize_text(row['responsibilities'])}\n",
    "# \"\"\"\n",
    "\n",
    "# # =====================================================================\n",
    "# # Stage 1: Embedding coarse filter\n",
    "# # =====================================================================\n",
    "# def embed_text(text: str):\n",
    "#     return embedder.encode(text if isinstance(text, str) else \"\")\n",
    "\n",
    "# print(\"Building JD embeddings...\")\n",
    "# jd_texts = jd_df[\"job_description\"].fillna(\"\").tolist()\n",
    "# jd_embeddings = embedder.encode(jd_texts, show_progress_bar=True)\n",
    "\n",
    "# def fast_filter_candidates(resume_text, top_k=20):\n",
    "#     emb = embed_text(resume_text).reshape(1, -1)\n",
    "#     sims = cosine_similarity(emb, jd_embeddings)[0]\n",
    "#     top_ids = np.argsort(sims)[::-1][:top_k]\n",
    "#     return jd_df.iloc[top_ids]\n",
    "\n",
    "# # =====================================================================\n",
    "# # LLM scoring\n",
    "# # =====================================================================\n",
    "# LLM_MODEL = \"phi3:mini\"\n",
    "\n",
    "# def build_prompt(resume_text, jd_batch):\n",
    "#     jd_section = \"\"\n",
    "#     for _, row in jd_batch.iterrows():\n",
    "#         desc = summarize_text(row[\"job_description\"], max_length=900)\n",
    "#         jd_section += f\"\"\"\n",
    "# ---\n",
    "# Job ID: {row['job_id']}\n",
    "# Job Title: {row['job_title']}\n",
    "# Location: {row['location_cleaned']}\n",
    "# Description:\n",
    "# {desc}\n",
    "# \"\"\"\n",
    "#     return f\"\"\"\n",
    "# You are a senior hiring expert.\n",
    "\n",
    "# Score each job from 0‚Äì1 based ONLY on:\n",
    "# 1. Skills match\n",
    "# 2. Work experience match\n",
    "# 3. Education match\n",
    "# 4. Location match (lowest weight)\n",
    "\n",
    "# Return STRICT JSON ONLY:\n",
    "# [\n",
    "#   {{\"job_id\": 0, \"score\": 0.00}}\n",
    "# ]\n",
    "\n",
    "# Resume:\n",
    "# {resume_text}\n",
    "\n",
    "# Job Descriptions:\n",
    "# {jd_section}\n",
    "\n",
    "# Return TOP 5.\n",
    "# \"\"\"\n",
    "\n",
    "# def score_batch_with_llm(resume_text, jd_batch, retry=3):\n",
    "#     prompt = build_prompt(resume_text, jd_batch)\n",
    "#     for _ in range(retry):\n",
    "#         try:\n",
    "#             resp = ollama.chat(\n",
    "#                 model=LLM_MODEL,\n",
    "#                 messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "#                 options={\"num_ctx\": 4096}\n",
    "#             )\n",
    "#             txt = resp[\"message\"][\"content\"]\n",
    "#             data = safe_json_loads(txt)\n",
    "#             if data:\n",
    "#                 return data\n",
    "#         except:\n",
    "#             time.sleep(1)\n",
    "#     return []\n",
    "\n",
    "# # =====================================================================\n",
    "# # GUARANTEE top1 exists\n",
    "# # =====================================================================\n",
    "# def ensure_top1(scored):\n",
    "#     return scored[:1] if len(scored) >= 1 else [(0, 0.0)]\n",
    "\n",
    "# # =====================================================================\n",
    "# # Match per resume\n",
    "# # =====================================================================\n",
    "# def chunk_df(df, size=10):\n",
    "#     for i in range(0, len(df), size):\n",
    "#         yield df.iloc[i:i+size]\n",
    "\n",
    "# def match_resume_to_jobs(resume_text):\n",
    "#     filtered = fast_filter_candidates(resume_text, top_k=20)\n",
    "#     scored = {}\n",
    "\n",
    "#     with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "#         futures = [\n",
    "#             executor.submit(score_batch_with_llm, resume_text, batch)\n",
    "#             for batch in chunk_df(filtered, size=10)\n",
    "#         ]\n",
    "#         for fut in as_completed(futures):\n",
    "#             for r in fut.result():\n",
    "#                 try:\n",
    "#                     jid = int(r[\"job_id\"])\n",
    "#                     score = float(r[\"score\"])\n",
    "#                     if jid in VALID_JOB_IDS:\n",
    "#                         scored[jid] = max(scored.get(jid, 0.0), score)\n",
    "#                 except:\n",
    "#                     continue\n",
    "\n",
    "#     ranked = sorted(scored.items(), key=lambda x: x[1], reverse=True)\n",
    "#     return ensure_top1(ranked)\n",
    "\n",
    "# # =====================================================================\n",
    "# # Run all resumes\n",
    "# # =====================================================================\n",
    "# output = []\n",
    "\n",
    "# for _, row in tqdm(resume_df.iterrows(), total=len(resume_df), desc=\"Resumes\"):\n",
    "#     rid = row[\"resume_id\"]\n",
    "#     resume_text = build_resume_profile(row)\n",
    "\n",
    "#     top1 = match_resume_to_jobs(resume_text)\n",
    "#     jid, score = top1[0]\n",
    "\n",
    "#     job_row = jd_df[jd_df[\"job_id\"] == jid].iloc[0] if jid in VALID_JOB_IDS else jd_df.iloc[0]\n",
    "\n",
    "#     output.append({\n",
    "#         \"resume_id\": rid,\n",
    "#         \"top1_job_id\": jid,\n",
    "#         \"top1_job_title\": job_row[\"job_title\"],\n",
    "#         \"top1_job_location\": job_row[\"location_cleaned\"],\n",
    "#         \"top1_score\": score\n",
    "#     })\n",
    "\n",
    "# pd.DataFrame(output).to_excel(\"resume_job_groundtrue.xlsx\", index=False)\n",
    "# print(\"\\nDONE! Saved to resume_job_groundtrue.xlsx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fbd14f",
   "metadata": {},
   "source": [
    "# step 2 only for missing/less performance in step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461dc3c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Will match resume IDs: [29, 82, 83, 86, 94]\n",
      "\n",
      "üéâ DONE! Saved to resume_job_special_rematch.xlsx \n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import re\n",
    "# import ast\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# # ============================\n",
    "# # Load Model\n",
    "# # ============================\n",
    "# embedder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# # ============================\n",
    "# # Load JD and Resume Data\n",
    "# # ============================\n",
    "# jd_df = pd.read_excel(\"../1_data_cleaning/filtered_jd_sections2.xlsx\")\n",
    "# jd_df[\"job_id\"] = jd_df.index.astype(int)\n",
    "\n",
    "# resume_df = pd.read_csv(\"../1_data_cleaning/resume_cleaned_100.csv\")\n",
    "# resume_df = resume_df[\n",
    "#     [\n",
    "#         \"career_objective\",\n",
    "#         \"skills\",\n",
    "#         \"degree_names\",\n",
    "#         \"major_field_of_studies\",\n",
    "#         \"positions\",\n",
    "#         \"responsibilities\",\n",
    "#     ]\n",
    "# ]\n",
    "\n",
    "# # ---------------------------\n",
    "# # Build FULL resume index\n",
    "# # ---------------------------\n",
    "# resume_df = resume_df.reset_index(drop=True)\n",
    "# resume_df[\"resume_id\"] = resume_df.index.astype(int)\n",
    "\n",
    "# # ---------------------------\n",
    "# # SPECIAL IDs \n",
    "# # ---------------------------\n",
    "# special_manual = [29,82,83,86,94]\n",
    "# # special_range = list(range(80, 100))\n",
    "\n",
    "# SPECIAL_IDS = sorted(list(set(special_manual)))\n",
    "\n",
    "# print(\"üîç Will match resume IDs:\", SPECIAL_IDS)\n",
    "\n",
    "# # ============================\n",
    "# # Utility Functions\n",
    "# # ============================\n",
    "# def clean_text(x):\n",
    "#     if isinstance(x, float) or pd.isna(x):\n",
    "#         return \"\"\n",
    "#     x = str(x)\n",
    "#     x = re.sub(r\"\\s+\", \" \", x)\n",
    "#     return x.strip()\n",
    "\n",
    "# def build_resume_text(row):\n",
    "#     \"\"\"Merge all resume fields into one text block.\"\"\"\n",
    "#     return (\n",
    "#         clean_text(row[\"career_objective\"])\n",
    "#         + \" \"\n",
    "#         + clean_text(row[\"skills\"])\n",
    "#         + \" \"\n",
    "#         + clean_text(row[\"degree_names\"])\n",
    "#         + \" \"\n",
    "#         + clean_text(row[\"major_field_of_studies\"])\n",
    "#         + \" \"\n",
    "#         + clean_text(row[\"positions\"])\n",
    "#         + \" \"\n",
    "#         + clean_text(row[\"responsibilities\"])\n",
    "#     )\n",
    "\n",
    "# def extract_keywords(position_text):\n",
    "#     \"\"\"Parse list-like strings.\"\"\"\n",
    "#     if not isinstance(position_text, str):\n",
    "#         return []\n",
    "\n",
    "#     try:\n",
    "#         items = ast.literal_eval(position_text)\n",
    "#         if not isinstance(items, list):\n",
    "#             items = [items]\n",
    "#     except:\n",
    "#         items = [position_text]\n",
    "\n",
    "#     keywords = []\n",
    "#     for item in items:\n",
    "#         if not isinstance(item, str):\n",
    "#             continue\n",
    "#         words = item.lower().strip().split()\n",
    "#         words = [w for w in words if len(w) > 2]\n",
    "#         keywords.extend(words)\n",
    "\n",
    "#     return list(set(keywords))\n",
    "\n",
    "# def filter_jd_by_keywords(jd_df, keywords):\n",
    "#     if not keywords:\n",
    "#         return jd_df\n",
    "#     mask = jd_df[\"job_description\"].fillna(\"\").str.lower().apply(\n",
    "#         lambda x: any(kw in x for kw in keywords)\n",
    "#     )\n",
    "#     filtered = jd_df[mask]\n",
    "#     return filtered if len(filtered) > 0 else jd_df\n",
    "\n",
    "# # ============================\n",
    "# # Matching Logic\n",
    "# # ============================\n",
    "# def match_resume(resume_row):\n",
    "#     rid = resume_row[\"resume_id\"]\n",
    "#     resume_text = build_resume_text(resume_row)\n",
    "\n",
    "#     keywords = extract_keywords(resume_row[\"positions\"])\n",
    "#     candidate_jd = filter_jd_by_keywords(jd_df, keywords)\n",
    "\n",
    "#     resume_emb = embedder.encode(resume_text)\n",
    "#     jd_texts = candidate_jd[\"job_description\"].fillna(\"\").tolist()\n",
    "#     jd_embs = embedder.encode(jd_texts)\n",
    "\n",
    "#     sims = cosine_similarity(resume_emb.reshape(1, -1), jd_embs)[0]\n",
    "#     best_idx = np.argmax(sims)\n",
    "#     best_jd_row = candidate_jd.iloc[best_idx]\n",
    "\n",
    "#     return {\n",
    "#         \"resume_id\": rid,\n",
    "#         \"top1_job_id\": int(best_jd_row[\"job_id\"]),\n",
    "#         \"top1_job_title\": best_jd_row[\"job_title\"],\n",
    "#         \"top1_job_location\": best_jd_row[\"location_cleaned\"],\n",
    "#         \"top1_score\": float(sims[best_idx]),\n",
    "#     }\n",
    "\n",
    "\n",
    "# # ============================\n",
    "# # Run Matching for SPECIAL + 80‚Äì100\n",
    "# # ============================\n",
    "# results = []\n",
    "\n",
    "# for rid in SPECIAL_IDS:\n",
    "#     if rid not in resume_df[\"resume_id\"].values:\n",
    "#         print(f\"‚ö†Ô∏è Warning: resume_id {rid} not found. Skipped.\")\n",
    "#         continue\n",
    "\n",
    "#     row = resume_df[resume_df[\"resume_id\"] == rid].iloc[0]\n",
    "#     result = match_resume(row)\n",
    "#     results.append(result)\n",
    "\n",
    "# output_df = pd.DataFrame(results)\n",
    "# output_df.to_excel(\"resume_job_special_rematch.xlsx\", index=False)\n",
    "\n",
    "# print(\"\\nüéâ DONE! Saved to resume_job_special_rematch.xlsx \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a197df78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building JD embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:05<00:00,  7.24it/s]\n",
      "Resumes:  24%|‚ñà‚ñà‚ñç       | 24/100 [38:35<2:57:56, 140.49s/it]"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import json\n",
    "# import numpy as np\n",
    "# from tqdm import tqdm\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "# import time\n",
    "# import ollama\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# # ========== Embedding model ==========\n",
    "# embedder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# # ========== Load data ==========\n",
    "# jd_df = pd.read_excel(\"jobdescription.xlsx\")\n",
    "# jd_df[\"job_id\"] = jd_df.index.astype(int)\n",
    "\n",
    "# resume_df = pd.read_csv(\"resume_cleaned_100.csv\")\n",
    "# resume_df = resume_df[['career_objective', 'skills', 'degree_names', 'major_field_of_studies',\n",
    "#        'positions', 'responsibilities']]\n",
    "\n",
    "# # =====================================================================\n",
    "# # Utility: robust JSON loader\n",
    "# # =====================================================================\n",
    "# def safe_json_loads(text):\n",
    "#     try:\n",
    "#         data = json.loads(text)\n",
    "#         if isinstance(data, dict):\n",
    "#             data = [data]\n",
    "#         if isinstance(data, list):\n",
    "#             return data\n",
    "#         return []\n",
    "#     except:\n",
    "#         try:\n",
    "#             start = text.index(\"[\")\n",
    "#             end = text.rindex(\"]\") + 1\n",
    "#             return json.loads(text[start:end])\n",
    "#         except:\n",
    "#             return []\n",
    "\n",
    "# # =====================================================================\n",
    "# # Text summarizer\n",
    "# # =====================================================================\n",
    "# def summarize_text(text, max_length=900):\n",
    "#     if not isinstance(text, str):\n",
    "#         return \"\"\n",
    "#     text = text.strip()\n",
    "#     if len(text) <= max_length:\n",
    "#         return text\n",
    "#     return text[:max_length]\n",
    "\n",
    "# # =====================================================================\n",
    "# # Build Resume text (NEW for your dataset)\n",
    "# # =====================================================================\n",
    "# def build_resume_profile(row):\n",
    "#     return f\"\"\"\n",
    "# Resume Text:\n",
    "# {summarize_text(row['Resume_str'])}\n",
    "\n",
    "# Category:\n",
    "# {row['Category']}\n",
    "# \"\"\"\n",
    "\n",
    "# # =====================================================================\n",
    "# # Stage 1: Embedding coarse filter\n",
    "# # =====================================================================\n",
    "# def embed_text(text: str):\n",
    "#     return embedder.encode(text if isinstance(text, str) else \"\")\n",
    "\n",
    "# print(\"Building JD embeddings...\")\n",
    "# jd_texts = jd_df[\"job_description\"].fillna(\"\").tolist()\n",
    "# jd_embeddings = embedder.encode(jd_texts, show_progress_bar=True)\n",
    "\n",
    "# def fast_filter_candidates(resume_text, top_k=20):\n",
    "#     emb = embed_text(resume_text).reshape(1, -1)\n",
    "#     sims = cosine_similarity(emb, jd_embeddings)[0]\n",
    "#     top_ids = np.argsort(sims)[::-1][:top_k]\n",
    "#     return jd_df.iloc[top_ids]\n",
    "\n",
    "# # =====================================================================\n",
    "# # LLM scoring\n",
    "# # =====================================================================\n",
    "# LLM_MODEL = \"phi3:mini\"\n",
    "\n",
    "# def build_prompt(resume_text, jd_batch):\n",
    "#     jd_section = \"\"\n",
    "#     for _, row in jd_batch.iterrows():\n",
    "#         desc = summarize_text(row[\"job_description\"], max_length=900)\n",
    "#         jd_section += f\"\"\"\n",
    "# ---\n",
    "# Job ID: {row['job_id']}\n",
    "# Job Title: {row['job_title']}\n",
    "# Location: {row['location_cleaned']}\n",
    "# Description:\n",
    "# {desc}\n",
    "# \"\"\"\n",
    "#     return f\"\"\"\n",
    "# You are a senior hiring expert.\n",
    "\n",
    "# Score each job from 0‚Äì1 based ONLY on:\n",
    "# 1. Skills match\n",
    "# 2. Work experience match\n",
    "# 3. Education match\n",
    "# 4. Location\n",
    "\n",
    "# Output STRICT JSON ONLY:\n",
    "# [\n",
    "#   {{\"job_id\": 0, \"score\": 0.00}}\n",
    "# ]\n",
    "\n",
    "# Resume:\n",
    "# {resume_text}\n",
    "\n",
    "# Job Descriptions:\n",
    "# {jd_section}\n",
    "\n",
    "# Return TOP 5.\n",
    "# \"\"\"\n",
    "\n",
    "# def score_batch_with_llm(resume_text, jd_batch, retry=3):\n",
    "#     prompt = build_prompt(resume_text, jd_batch)\n",
    "#     for _ in range(retry):\n",
    "#         try:\n",
    "#             resp = ollama.chat(\n",
    "#                 model=LLM_MODEL,\n",
    "#                 messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "#                 options={\"num_ctx\": 4096}\n",
    "#             )\n",
    "#             txt = resp[\"message\"][\"content\"]\n",
    "#             data = safe_json_loads(txt)\n",
    "#             if data:\n",
    "#                 return data\n",
    "#         except:\n",
    "#             time.sleep(1)\n",
    "#     return []\n",
    "\n",
    "# # =====================================================================\n",
    "# # GUARANTEE top1 exists\n",
    "# # =====================================================================\n",
    "# def ensure_top1(scored, jd_df):\n",
    "#     if len(scored) >= 1:\n",
    "#         return scored[:1]\n",
    "#     fallback_id = int(jd_df.iloc[0][\"job_id\"])\n",
    "#     return [(fallback_id, 0.0)]\n",
    "\n",
    "# # =====================================================================\n",
    "# # Match per resume\n",
    "# # =====================================================================\n",
    "# def chunk_df(df, size=10):\n",
    "#     for i in range(0, len(df), size):\n",
    "#         yield df.iloc[i:i+size]\n",
    "\n",
    "# def match_resume_to_jobs(resume_text, jd_df):\n",
    "#     filtered = fast_filter_candidates(resume_text, top_k=20)\n",
    "#     scored = {}\n",
    "\n",
    "#     with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "#         futures = [\n",
    "#             executor.submit(score_batch_with_llm, resume_text, batch)\n",
    "#             for batch in chunk_df(filtered, size=10)\n",
    "#         ]\n",
    "#         for fut in as_completed(futures):\n",
    "#             results = fut.result()\n",
    "#             for r in results:\n",
    "#                 try:\n",
    "#                     jid = int(r[\"job_id\"])\n",
    "#                     score = float(r[\"score\"])\n",
    "#                     scored[jid] = max(scored.get(jid, 0.0), score)\n",
    "#                 except:\n",
    "#                     continue\n",
    "\n",
    "#     if not scored:\n",
    "#         return ensure_top1([], jd_df)\n",
    "\n",
    "#     ranked = sorted(scored.items(), key=lambda x: x[1], reverse=True)\n",
    "#     return ensure_top1(ranked, jd_df)\n",
    "\n",
    "# # =====================================================================\n",
    "# # Run all resumes\n",
    "# # =====================================================================\n",
    "# output = []\n",
    "\n",
    "# for _, row in tqdm(resume_df.iterrows(), total=len(resume_df), desc=\"Resumes\"):\n",
    "#     rid = row[\"ID\"]\n",
    "#     resume_text = build_resume_profile(row)\n",
    "\n",
    "#     top1 = match_resume_to_jobs(resume_text, jd_df)\n",
    "\n",
    "#     jid, score = top1[0]\n",
    "#     job_row = jd_df[jd_df[\"job_id\"] == jid].iloc[0]\n",
    "\n",
    "#     rec = {\n",
    "#         \"resume_id\": rid,\n",
    "#         \"top1_job_id\": jid,\n",
    "#         \"top1_job_title\": job_row[\"job_title\"],\n",
    "#         \"top1_job_location\": job_row[\"location_cleaned\"],\n",
    "#         \"top1_score\": score\n",
    "#     }\n",
    "#     output.append(rec)\n",
    "\n",
    "# pd.DataFrame(output).to_excel(\"resume_job_groundtrue.xlsx\", index=False)\n",
    "# print(\"\\nDONE! Saved to resume_job_groundtrue.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5945eafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer')), '(Request ID: 114ad6d4-4e87-434b-8eba-2446435aec9c)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./modules.json\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building JD embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:05<00:00,  7.45it/s]\n",
      "Resumes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [07:57<00:00, 47.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DONE! Saved to resume_job_matching_results.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import json\n",
    "# import numpy as np\n",
    "# from tqdm import tqdm\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "# import time\n",
    "# import ollama\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# # ========== Embedding model ==========\n",
    "# embedder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# # ========== Load data ==========\n",
    "# jd_df = pd.read_excel(\"jobdescription.xlsx\")\n",
    "# jd_df[\"job_id\"] = jd_df.index.astype(int)\n",
    "\n",
    "# resume_df = pd.read_csv(\"Resume.csv\")\n",
    "# resume_df = resume_df[\n",
    "#     [ \"ID\",\n",
    "#     \"Resume_str\", \"Category\"]\n",
    "# ]\n",
    "\n",
    "# # =====================================================================\n",
    "# # Utility: robust JSON loader\n",
    "# # =====================================================================\n",
    "# def safe_json_loads(text):\n",
    "#     try:\n",
    "#         data = json.loads(text)\n",
    "#         if isinstance(data, dict):\n",
    "#             data = [data]\n",
    "#         if isinstance(data, list):\n",
    "#             return data\n",
    "#         return []\n",
    "#     except:\n",
    "#         try:\n",
    "#             start = text.index(\"[\")\n",
    "#             end = text.rindex(\"]\") + 1\n",
    "#             return json.loads(text[start:end])\n",
    "#         except:\n",
    "#             return []\n",
    "\n",
    "# # =====================================================================\n",
    "# # Text summarizer\n",
    "# # =====================================================================\n",
    "# def summarize_text(text, max_length=900):\n",
    "#     if not isinstance(text, str):\n",
    "#         return \"\"\n",
    "#     text = text.strip()\n",
    "#     if len(text) <= max_length:\n",
    "#         return text\n",
    "\n",
    "#     lines = text.split(\"\\n\")\n",
    "#     summary = []\n",
    "#     for ln in lines:\n",
    "#         ln = ln.strip()\n",
    "#         if len(ln) > 20:\n",
    "#             summary.append(ln)\n",
    "#         if len(\" \".join(summary)) > max_length:\n",
    "#             break\n",
    "\n",
    "#     return \" \".join(summary)[:max_length]\n",
    "\n",
    "# # =====================================================================\n",
    "# # Build Resume text\n",
    "# # =====================================================================\n",
    "# def build_resume_profile(row):\n",
    "#     return f\"\"\"\n",
    "# Resume Title: {row['Resume Title']}\n",
    "\n",
    "# Work Experience:\n",
    "# {summarize_text(row['Work Experience'])}\n",
    "\n",
    "# Education:\n",
    "# {summarize_text(row['Education'])}\n",
    "\n",
    "# Skills:\n",
    "# {summarize_text(row['Skills'])}\n",
    "\n",
    "# Additional Information:\n",
    "# {summarize_text(row['Additional Information'])}\n",
    "# \"\"\"\n",
    "\n",
    "# # =====================================================================\n",
    "# # Stage 1: Embedding coarse filter\n",
    "# # =====================================================================\n",
    "# def embed_text(text: str):\n",
    "#     return embedder.encode(text if isinstance(text, str) else \"\")\n",
    "\n",
    "# print(\"Building JD embeddings...\")\n",
    "# jd_texts = jd_df[\"job_description\"].fillna(\"\").tolist()\n",
    "# jd_embeddings = embedder.encode(jd_texts, show_progress_bar=True)\n",
    "\n",
    "# def fast_filter_candidates(resume_text, top_k=20):\n",
    "#     emb = embed_text(resume_text).reshape(1, -1)\n",
    "#     sims = cosine_similarity(emb, jd_embeddings)[0]\n",
    "#     top_ids = np.argsort(sims)[::-1][:top_k]\n",
    "#     return jd_df.iloc[top_ids]\n",
    "\n",
    "# # =====================================================================\n",
    "# # LLM scoring\n",
    "# # =====================================================================\n",
    "# LLM_MODEL = \"phi3:mini\"\n",
    "\n",
    "# def build_prompt(resume_text, jd_batch):\n",
    "#     jd_section = \"\"\n",
    "#     for _, row in jd_batch.iterrows():\n",
    "#         desc = summarize_text(row[\"job_description\"], max_length=900)\n",
    "#         jd_section += f\"\"\"\n",
    "# ---\n",
    "# Job ID: {row['job_id']}\n",
    "# Job Title: {row['job_title']}\n",
    "# Location: {row['location_cleaned']}\n",
    "# Description:\n",
    "# {desc}\n",
    "# \"\"\"\n",
    "\n",
    "#     return f\"\"\"\n",
    "# You are a senior hiring expert.\n",
    "\n",
    "# Score each job from 0‚Äì1 based ONLY on:\n",
    "# 1. Skills match\n",
    "# 2. Work experience match\n",
    "# 3. Education match\n",
    "# 4. Location\n",
    "\n",
    "# Output STRICT JSON ONLY:\n",
    "# [\n",
    "#   {{\"job_id\": 0, \"score\": 0.00}}\n",
    "# ]\n",
    "\n",
    "# NO explanations.\n",
    "\n",
    "# Resume:\n",
    "# {resume_text}\n",
    "\n",
    "# Job Descriptions:\n",
    "# {jd_section}\n",
    "\n",
    "# Return TOP 5.\n",
    "# \"\"\"\n",
    "\n",
    "# def score_batch_with_llm(resume_text, jd_batch, retry=3):\n",
    "#     prompt = build_prompt(resume_text, jd_batch)\n",
    "\n",
    "#     for _ in range(retry):\n",
    "#         try:\n",
    "#             resp = ollama.chat(\n",
    "#                 model=LLM_MODEL,\n",
    "#                 messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "#                 options={\"num_ctx\": 4096}\n",
    "#             )\n",
    "#             txt = resp[\"message\"][\"content\"]\n",
    "#             data = safe_json_loads(txt)\n",
    "#             if data:\n",
    "#                 return data\n",
    "#         except:\n",
    "#             time.sleep(1)\n",
    "\n",
    "#     return []\n",
    "\n",
    "# # =====================================================================\n",
    "# # GUARANTEE top1 exists\n",
    "# # =====================================================================\n",
    "# def ensure_top1(scored, jd_df):\n",
    "#     \"\"\"\n",
    "#     Only keep top1. If empty, fill with dummy job.\n",
    "#     \"\"\"\n",
    "#     if len(scored) >= 1:\n",
    "#         return scored[:1]\n",
    "\n",
    "#     # pick any job as filler\n",
    "#     fallback_id = int(jd_df.iloc[0][\"job_id\"])\n",
    "#     return [(fallback_id, 0.0)]\n",
    "\n",
    "# # =====================================================================\n",
    "# # Match per resume\n",
    "# # =====================================================================\n",
    "# def chunk_df(df, size=10):\n",
    "#     for i in range(0, len(df), size):\n",
    "#         yield df.iloc[i:i+size]\n",
    "\n",
    "# def match_resume_to_jobs(resume_text, jd_df):\n",
    "#     filtered = fast_filter_candidates(resume_text, top_k=20)\n",
    "#     scored = {}\n",
    "\n",
    "#     with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "#         futures = [\n",
    "#             executor.submit(score_batch_with_llm, resume_text, batch)\n",
    "#             for batch in chunk_df(filtered, size=10)\n",
    "#         ]\n",
    "#         for fut in as_completed(futures):\n",
    "#             results = fut.result()\n",
    "#             for r in results:\n",
    "#                 try:\n",
    "#                     jid = int(r[\"job_id\"])\n",
    "#                     score = float(r[\"score\"])\n",
    "#                     scored[jid] = max(scored.get(jid, 0.0), score)\n",
    "#                 except:\n",
    "#                     continue\n",
    "\n",
    "#     if not scored:\n",
    "#         return ensure_top1([], jd_df)\n",
    "\n",
    "#     ranked = sorted(scored.items(), key=lambda x: x[1], reverse=True)\n",
    "#     return ensure_top1(ranked, jd_df)\n",
    "\n",
    "# # =====================================================================\n",
    "# # Run all resumes\n",
    "# # =====================================================================\n",
    "# output = []\n",
    "\n",
    "# for _, row in tqdm(resume_df.iterrows(), total=len(resume_df), desc=\"Resumes\"):\n",
    "#     rid = row[\"Uniq Id\"]\n",
    "#     resume_text = build_resume_profile(row)\n",
    "\n",
    "#     top1 = match_resume_to_jobs(resume_text, jd_df)\n",
    "\n",
    "#     jid, score = top1[0]\n",
    "#     job_row = jd_df[jd_df[\"job_id\"] == jid].iloc[0]\n",
    "\n",
    "#     rec = {\n",
    "#         \"resume_id\": rid,\n",
    "#         \"top1_job_id\": jid,\n",
    "#         \"top1_job_title\": job_row[\"job_title\"],\n",
    "#         \"top1_job_location\": job_row[\"location_cleaned\"],\n",
    "#         \"top1_score\": score\n",
    "#     }\n",
    "\n",
    "#     output.append(rec)\n",
    "\n",
    "# pd.DataFrame(output).to_excel(\"resume_job_matching_results.xlsx\", index=False)\n",
    "# print(\"\\nDONE! Saved to resume_job_matching_results.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98de049d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building JD embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:05<00:00,  7.36it/s]\n",
      "Resumes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [07:50<00:00, 47.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DONE! Saved to resume_job_matching_results.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import json\n",
    "# import numpy as np\n",
    "# from tqdm import tqdm\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "# import time\n",
    "# import ollama\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# # ========== Embedding model ==========\n",
    "# embedder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# # ========== Load data ==========\n",
    "# jd_df = pd.read_excel(\"jobdescription.xlsx\")\n",
    "# jd_df[\"job_id\"] = jd_df.index.astype(int)\n",
    "\n",
    "# resume_df = pd.read_csv(\"resumes.csv\")\n",
    "# resume_df = resume_df[\n",
    "#     [\"Uniq Id\", \"Resume Title\", \"Introduction\",\n",
    "#      \"Work Experience\", \"Education\",\n",
    "#      \"Skills\", \"Additional Information\"]\n",
    "# ]\n",
    "\n",
    "# # =====================================================================\n",
    "# # Utility: robust JSON loader\n",
    "# # =====================================================================\n",
    "# def safe_json_loads(text):\n",
    "#     \"\"\"Never crash; always return [] or valid list.\"\"\"\n",
    "#     try:\n",
    "#         data = json.loads(text)\n",
    "#         if isinstance(data, dict):\n",
    "#             data = [data]\n",
    "#         if isinstance(data, list):\n",
    "#             return data\n",
    "#         return []\n",
    "#     except:\n",
    "#         # Try extract JSON part\n",
    "#         try:\n",
    "#             start = text.index(\"[\")\n",
    "#             end = text.rindex(\"]\") + 1\n",
    "#             return json.loads(text[start:end])\n",
    "#         except:\n",
    "#             return []\n",
    "\n",
    "# # =====================================================================\n",
    "# # Text summarizer to reduce prompt size\n",
    "# # =====================================================================\n",
    "# def summarize_text(text, max_length=900):\n",
    "#     \"\"\"Shorten long resume or job description.\"\"\"\n",
    "#     if not isinstance(text, str):\n",
    "#         return \"\"\n",
    "\n",
    "#     text = text.strip()\n",
    "#     if len(text) <= max_length:\n",
    "#         return text\n",
    "\n",
    "#     # Simple extraction (fast)\n",
    "#     lines = text.split(\"\\n\")\n",
    "#     summary = []\n",
    "#     for ln in lines:\n",
    "#         ln = ln.strip()\n",
    "#         if len(ln) > 20:\n",
    "#             summary.append(ln)\n",
    "#         if len(\" \".join(summary)) > max_length:\n",
    "#             break\n",
    "\n",
    "#     return \" \".join(summary)[:max_length]\n",
    "\n",
    "# # =====================================================================\n",
    "# # Build Resume text with summarization\n",
    "# # =====================================================================\n",
    "# def build_resume_profile(row):\n",
    "#     return f\"\"\"\n",
    "# Resume Title: {row['Resume Title']}\n",
    "\n",
    "# Work Experience:\n",
    "# {summarize_text(row['Work Experience'])}\n",
    "\n",
    "# Education:\n",
    "# {summarize_text(row['Education'])}\n",
    "\n",
    "# Skills:\n",
    "# {summarize_text(row['Skills'])}\n",
    "\n",
    "# Additional Information:\n",
    "# {summarize_text(row['Additional Information'])}\n",
    "# \"\"\"\n",
    "\n",
    "# # =====================================================================\n",
    "# # Stage 1: Embedding coarse filter\n",
    "# # =====================================================================\n",
    "# def embed_text(text: str) -> np.ndarray:\n",
    "#     return embedder.encode(text if isinstance(text, str) else \"\")\n",
    "\n",
    "# print(\"Building JD embeddings...\")\n",
    "# jd_texts = jd_df[\"job_description\"].fillna(\"\").tolist()\n",
    "# jd_embeddings = embedder.encode(jd_texts, show_progress_bar=True)\n",
    "\n",
    "# def fast_filter_candidates(resume_text, top_k=20):\n",
    "#     emb = embed_text(resume_text).reshape(1, -1)\n",
    "#     sims = cosine_similarity(emb, jd_embeddings)[0]\n",
    "#     top_ids = np.argsort(sims)[::-1][:top_k]\n",
    "#     return jd_df.iloc[top_ids]\n",
    "\n",
    "# # =====================================================================\n",
    "# # Stable LLM scoring (with retry + summarization)\n",
    "# # =====================================================================\n",
    "# LLM_MODEL = \"phi3:mini\"\n",
    "\n",
    "# def build_prompt(resume_text, jd_batch):\n",
    "#     jd_section = \"\"\n",
    "#     for _, row in jd_batch.iterrows():\n",
    "#         # compress JD text\n",
    "#         desc = summarize_text(row[\"job_description\"], max_length=900)\n",
    "#         jd_section += f\"\"\"\n",
    "# ---\n",
    "# Job ID: {row['job_id']}\n",
    "# Job Title: {row['job_title']}\n",
    "# Location: {row['location_cleaned']}\n",
    "# Description:\n",
    "# {desc}\n",
    "# \"\"\"\n",
    "\n",
    "#     return f\"\"\"\n",
    "# You are a senior hiring expert.\n",
    "\n",
    "# Score each job from 0‚Äì1 based ONLY on:\n",
    "# 1. Skills match (highest weight)\n",
    "# 2. Work experience match\n",
    "# 3. Education match\n",
    "# 4. Location match (lowest weight)\n",
    "\n",
    "# Output STRICT JSON ONLY in this exact format:\n",
    "# [\n",
    "#   {{\"job_id\": 0, \"score\": 0.00}}\n",
    "# ]\n",
    "\n",
    "# NO explanations. NO extra text.\n",
    "\n",
    "# Resume:\n",
    "# {resume_text}\n",
    "\n",
    "# Job Descriptions:\n",
    "# {jd_section}\n",
    "\n",
    "# Return TOP 5 highest scoring entries.\n",
    "# \"\"\"\n",
    "\n",
    "# def score_batch_with_llm(resume_text, jd_batch, retry=3):\n",
    "#     prompt = build_prompt(resume_text, jd_batch)\n",
    "\n",
    "#     for _ in range(retry):\n",
    "#         try:\n",
    "#             resp = ollama.chat(\n",
    "#                 model=LLM_MODEL,\n",
    "#                 messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "#                 options={\"num_ctx\": 4096}\n",
    "#             )\n",
    "#             txt = resp[\"message\"][\"content\"]\n",
    "#             data = safe_json_loads(txt)\n",
    "#             if data:\n",
    "#                 return data\n",
    "        \n",
    "#         except Exception:\n",
    "#             time.sleep(1)\n",
    "\n",
    "#     return []  # fallback\n",
    "\n",
    "# # =====================================================================\n",
    "# # Guarantee top1/top2/top3 always exist\n",
    "# # =====================================================================\n",
    "# def ensure_top3(scored, jd_df):\n",
    "#     if len(scored) >= 3:\n",
    "#         return scored[:3]\n",
    "\n",
    "#     # fill missing with dummy lowest score\n",
    "#     needed = 3 - len(scored)\n",
    "\n",
    "#     all_job_ids = set(jd_df[\"job_id\"])\n",
    "#     used = {jid for jid, _ in scored}\n",
    "#     remaining = list(all_job_ids - used)[:needed]\n",
    "\n",
    "#     for jid in remaining:\n",
    "#         scored.append((jid, 0.0))\n",
    "\n",
    "#     return scored[:3]\n",
    "\n",
    "# # =====================================================================\n",
    "# # Master function for each resume\n",
    "# # =====================================================================\n",
    "# def chunk_df(df, size=10):\n",
    "#     for i in range(0, len(df), size):\n",
    "#         yield df.iloc[i:i+size]\n",
    "\n",
    "# def match_resume_to_jobs(resume_text, jd_df):\n",
    "#     filtered = fast_filter_candidates(resume_text, top_k=20)\n",
    "#     scored = {}\n",
    "\n",
    "#     with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "#         futures = [\n",
    "#             executor.submit(score_batch_with_llm, resume_text, batch)\n",
    "#             for batch in chunk_df(filtered, size=10)\n",
    "#         ]\n",
    "#         for fut in as_completed(futures):\n",
    "#             results = fut.result()\n",
    "#             for r in results:\n",
    "#                 try:\n",
    "#                     jid = int(r[\"job_id\"])\n",
    "#                     score = float(r[\"score\"])\n",
    "#                     scored[jid] = max(scored.get(jid, 0.0), score)\n",
    "#                 except:\n",
    "#                     continue\n",
    "\n",
    "#     if not scored:\n",
    "#         return ensure_top3([], jd_df)\n",
    "\n",
    "#     ranked = sorted(scored.items(), key=lambda x: x[1], reverse=True)\n",
    "#     return ensure_top3(ranked, jd_df)\n",
    "\n",
    "# # =====================================================================\n",
    "# # Run all\n",
    "# # =====================================================================\n",
    "# output = []\n",
    "\n",
    "# for _, row in tqdm(resume_df.iterrows(), total=len(resume_df), desc=\"Resumes\"):\n",
    "#     rid = row[\"Uniq Id\"]\n",
    "#     resume_text = build_resume_profile(row)\n",
    "#     top3 = match_resume_to_jobs(resume_text, jd_df)\n",
    "\n",
    "#     rec = {\"resume_id\": rid}\n",
    "#     for i, (jid, score) in enumerate(top3, start=1):\n",
    "#         job_row = jd_df[jd_df[\"job_id\"] == jid].iloc[0]\n",
    "#         rec[f\"top{i}_job_id\"] = jid\n",
    "#         rec[f\"top{i}_job_title\"] = job_row[\"job_title\"]\n",
    "#         rec[f\"top{i}_job_location\"] = job_row[\"location_cleaned\"]\n",
    "#         rec[f\"top{i}_score\"] = score\n",
    "\n",
    "#     output.append(rec)\n",
    "\n",
    "# pd.DataFrame(output).to_excel(\"resume_job_matching_results.xlsx\", index=False)\n",
    "# print(\"\\nDONE! Saved to resume_job_matching_results.xlsx\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ollama39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
